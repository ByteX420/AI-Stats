# AI Gateway Executors - Complete Provider Coverage (2026)

This document outlines all provider executors implemented in the AI Gateway, organized by provider (capability subfolders).

## ğŸ“Š Summary

- **Total Text Generation Providers**: 28 providers (including Google Nano Banana multimodal models)
- **Image Generation**: OpenAI (DALL-E 3, DALL-E 2), Fal.ai (600+ models)
- **Video Generation**: Google (Veo 3.1, Veo 3.1 Fast, Veo 2), Fal.ai (Veo 3, Hunyuan, MiniMax, etc.)
- **Audio Generation**: OpenAI (TTS-1, TTS-1-HD, GPT-4o-mini-TTS), Fal.ai (TTS, music generation)

---

## ğŸ¤– Text Generation Executors (`text.generate`)

### Core Providers

| Provider | Status | Documentation | Notes |
|----------|--------|---------------|-------|
| **OpenAI** | âœ… Active | [Docs](https://platform.openai.com/docs) | GPT-4, GPT-4o, o1, o3 with reasoning support |
| **Anthropic** | âœ… Active | [Docs](https://docs.anthropic.com/) | Claude 3.5 Sonnet, Opus with extended thinking |
| **Google** | âœ… Active | [Docs](https://ai.google.dev/) | **Native implementation** - Gemini 2.0, 1.5 Pro/Flash with thinking mode + **Nano Banana** image-generating models |
| **Google AI Studio** | âœ… Active | [Docs](https://ai.google.dev/gemini-api) | Same as Google provider (native Gemini API) |

#### Google Gemini - Native Implementation

**Important:** Google Gemini uses a **native implementation**, not OpenAI-compatible format. Key differences:

- **Model in URL**: Model is part of the URL path (`/v1beta/models/{model}:generateContent`), not request body
- **Request Structure**: Uses `contents` array with `parts` (not `messages`)
- **Response Structure**: Returns `candidates` with `content.parts` (not `choices`)
- **System Messages**: Separate `systemInstruction` field (not in contents)
- **Parameters**: `generationConfig` with `maxOutputTokens`, `topK`, etc.
- **Thinking Mode**: Uses `thinkingBudget.tokens` in `generationConfig`

#### Google Nano Banana Models (Multimodal Output)

**Special Feature:** Google's Nano Banana models (`gemini-2.5-flash-image`, `gemini-3-pro-image-preview`) are unique text.generate models that can output **both text and images** in a single response:

- **Text parts** - Regular text output
- **Image parts** - Generated images as base64-encoded `inline_data`
- **Thought signatures** - For preserving reasoning context across multi-turn conversations

Example prompts:
- "Create a logo for my coffee shop and explain the design choices"
- "Generate a diagram of how photosynthesis works with detailed annotations"

The gateway automatically handles the mixed-media responses through native Google API parsing and IR conversion.

### High-Performance Providers

| Provider | Status | Documentation | Speed | Notes |
|----------|--------|---------------|-------|-------|
| **Groq** | âœ… NEW | [Docs](https://console.groq.com/docs/openai) | ğŸš€ Ultra-fast | LPU-powered, OpenAI-compatible |
| **Cerebras** | âœ… NEW | [Docs](https://inference-docs.cerebras.ai/) | ğŸš€ 3000+ tok/s | Wafer-Scale Engine |
| **Together** | âœ… NEW | [Docs](https://docs.together.ai/) | âš¡ Fast | 200+ open models |
| **Fireworks** | âœ… NEW | [Docs](https://docs.fireworks.ai/) | âš¡ Fast | Compound AI, MCP support |

### Chinese AI Providers

| Provider | Status | Documentation | Specialty |
|----------|--------|---------------|-----------|
| **DeepSeek** | âœ… Active | [Docs](https://api-docs.deepseek.com/) | Reasoning models (V3.2) |
| **Xiaomi MiMo** | âœ… Active | [GitHub](https://github.com/XiaomiMiMo/MiMo-V2-Flash) | MiMo-V2-Flash (309B MoE) |
| **MiniMax** | âœ… Active | [Docs](https://platform.minimax.io/) | M2.1 with interleaved thinking |
| **Z.AI (GLM)** | âœ… Active | [Docs](https://docs.z.ai/) | Zhipu AI GLM models |
| **Alibaba/Qwen** | âœ… NEW | [Docs](https://dashscope.aliyuncs.com/) | Qwen 2.5, QVQ, QwQ |
| **Moonshot AI** | âœ… NEW | [Docs](https://platform.moonshot.cn/) | Long context models |

### International Providers

| Provider | Status | Documentation | Specialty |
|----------|--------|---------------|-----------|
| **xAI** | âœ… Active | [Docs](https://docs.x.ai/) | Grok with real-time data |
| **Mistral** | âœ… NEW | [Docs](https://docs.mistral.ai/) | European AI, open models |
| **Cohere** | âœ… NEW | [Docs](https://docs.cohere.com/) | Enterprise RAG, embeddings |
| **Perplexity** | âœ… NEW | [Docs](https://docs.perplexity.ai/) | Search-augmented responses |
| **Liquid AI** | âœ… NEW | [Docs](https://docs.liquid.ai/) | Efficient reasoning models |

### Infrastructure Providers

| Provider | Status | Documentation | Specialty |
|----------|--------|---------------|-----------|
| **DeepInfra** | âœ… NEW | [Docs](https://deepinfra.com/docs) | Multi-cloud inference |
| **NovitaAI** | âœ… NEW | [Docs](https://docs.novita.ai/) | Global inference network |
| **Baseten** | âœ… NEW | [Docs](https://docs.baseten.co/) | ML infrastructure platform |
| **AION Labs** | âœ… Active | Internal | Custom models |

---

## ğŸ¨ Image Generation Executors (`image.generate`)

### Major Providers

| Provider | Status | Models Available | Documentation |
|----------|--------|------------------|---------------|
| **OpenAI** | âœ… NEW | DALL-E 3, DALL-E 2 | [Docs](https://platform.openai.com/docs/guides/images) |
| **Fal.ai** | âœ… NEW | 600+ models | [Docs](https://fal.ai/) |

#### OpenAI - DALL-E Models

- **DALL-E 3** (Scheduled deprecation: May 12, 2026):
  - Resolutions: 1024x1024, 1024x1792, 1792x1024
  - Quality: standard, hd
  - Style: vivid, natural
  - Single image per request (n=1 only)

- **DALL-E 2**:
  - Resolutions: 256x256, 512x512, 1024x1024
  - Lower cost, faster generation
  - Multiple images per request

#### Fal.ai - Unified Image API

- **FLUX Family**:
  - `fal-ai/flux/schnell` - Ultra-fast generation
  - `fal-ai/flux/dev` - High quality
  - `fal-ai/flux/pro` - Professional grade
- **Stable Diffusion**:
  - `fal-ai/stable-diffusion-v3`
  - `fal-ai/sd-xl`
  - `fal-ai/sd-turbo`
- **Specialty Models**:
  - Runway ML models
  - Midjourney-style models
  - And 590+ more

#### Example Usage

**OpenAI DALL-E 3:**
```bash
POST /v1/images/generations
{
  "model": "dall-e-3",
  "prompt": "A serene landscape with mountains",
  "size": "1024x1024",
  "quality": "hd",
  "style": "natural",
  "response_format": "url"
}
```

**Fal FLUX:**
```bash
POST /v1/images/generations
{
  "model": "fal-ai/flux/schnell",
  "prompt": "A serene landscape with mountains",
  "num_images": 4,
  "image_size": {
    "width": 1024,
    "height": 1024
  },
  "seed": 42
}
```

---

## ğŸ¬ Video Generation Executors (`video.generate`)

### Major Providers

| Provider | Status | Models Available | Documentation |
|----------|--------|------------------|---------------|
| **Google** | âœ… NEW | Veo 3.1, Veo 3.1 Fast, Veo 2 | [Docs](https://ai.google.dev/gemini-api/docs/video) |
| **Fal.ai** | âœ… NEW | 20+ models | [Docs](https://fal.ai/video) |

#### Google Veo - Native Implementation

- **Veo 3.1 Preview** (`veo-3.1-generate-preview`):
  - 720p, 1080p, 4k resolution support
  - 4, 6, or 8 second videos
  - Aspect ratios: 16:9, 9:16
  - Native audio generation
  - Text-to-video, image-to-video, video extension
  - Reference images (up to 3) for style/content guidance
  - Updated January 2026

- **Veo 3.1 Fast Preview** (`veo-3.1-fast-generate-preview`):
  - Speed-optimized version
  - Same capabilities as Veo 3.1

- **Veo 2 Stable** (`veo-2.0-generate-001`):
  - 720p only, no audio
  - Up to 2 videos per request

**Note:** Uses long-running operations - requires polling for completion

#### Fal.ai - Unified Video API

- **Google Veo 3**: `fal-ai/veo3` - Most advanced AI video model
- **Hunyuan Video**: `fal-ai/hunyuan-video` - Open source, high quality
- **MiniMax**: `fal-ai/minimax-video` - Text-to-video
- **Mochi V1**: `fal-ai/mochi-v1` - High quality generation
- **Kling Video**: `fal-ai/kling-video/v1/standard` - Image-to-video
- **Veo 2**: `fal-ai/veo2/image-to-video` - Google's image-to-video
- **Luma Dream Machine**: `fal-ai/luma-dream-machine`

#### Example Usage

**Google Veo 3.1:**
```bash
POST /v1/video/generation
{
  "model": "veo-3.1-generate-preview",
  "prompt": "A cat playing piano in a jazz club",
  "aspect_ratio": "16:9",
  "resolution": "1080p",
  "duration_seconds": 8,
  "negative_prompt": "blurry, low quality"
}
```

**Fal Veo 3:**
```bash
POST /v1/video/generation
{
  "model": "fal-ai/veo3",
  "prompt": "A cat playing piano in a jazz club",
  "duration_seconds": 5,
  "fps": 24,
  "aspect_ratio": "16:9",
  "seed": 42
}
```

#### Image-to-Video Example

```bash
POST /v1/video/generation
{
  "model": "fal-ai/veo2/image-to-video",
  "prompt": "The character waves at the camera",
  "image_url": "https://example.com/image.jpg",
  "duration_seconds": 3
}
```

---

## ğŸµ Audio Generation Executors (`audio.generate`)

### Major Providers

| Provider | Status | Capabilities | Documentation |
|----------|--------|--------------|---------------|
| **OpenAI** | âœ… NEW | TTS (tts-1, tts-1-hd, gpt-4o-mini-tts) | [Docs](https://platform.openai.com/docs/guides/text-to-speech) |
| **Fal.ai** | âœ… NEW | TTS, Music, Effects | [Docs](https://fal.ai/) |

#### OpenAI - Text-to-Speech

- **Models**:
  - `tts-1` - Standard quality, lower latency
  - `tts-1-hd` - High definition quality
  - `gpt-4o-mini-tts` - Latest model with instruction support
  - `gpt-4o-mini-tts-2025-12-15` - Dated snapshot

- **Voices**: alloy, ash, ballad, coral, echo, fable, onyx, nova, sage, shimmer, verse, marin, cedar
- **Formats**: mp3, opus, aac, flac, wav, pcm
- **Speed**: 0.25 to 4.0 (default 1.0)
- **Instructions**: Custom voice control (gpt-4o-mini-tts only)

#### Example Usage

**OpenAI TTS:**
```bash
POST /v1/audio/generation
{
  "model": "gpt-4o-mini-tts",
  "input": "Welcome to our service! Today is a wonderful day.",
  "voice": "coral",
  "instructions": "Speak in a cheerful and positive tone.",
  "response_format": "mp3",
  "speed": 1.0
}
```

**Fal Audio:**
```bash
POST /v1/audio/generation
{
  "model": "fal-ai/audio-tts",
  "prompt": "Welcome to our service!",
  "voice": "en-US-Neural-Female",
  "duration_seconds": 10
}
```

---

## ğŸ—ï¸ Architecture

### Executor Resolution Flow

```
Client Request
    â†“
Endpoint Detection (/v1/chat/completions, /v1/images/generations, etc.)
    â†“
Capability Mapping (text.generate, image.generate, video.generate, audio.generate)
    â†“
Provider Selection (based on model ID or gateway routing)
    â†“
Executor Resolution: resolveProviderExecutor(providerId, capability)
    â†“
Provider Execution (with IR transformation)
    â†“
Response Encoding (protocol-specific format)
    â†“
Client Response
```

### File Structure

```
apps/api/src/executors/
â”œâ”€â”€ index.ts                      # Main executor resolver
â”œâ”€â”€ types.ts                      # Shared types
â”œâ”€â”€ _shared/
â”‚   â””â”€â”€ text-generate/
â”‚       â”œâ”€â”€ shared.ts             # Common text.generate helpers
â”‚       â””â”€â”€ openai-compat/        # OpenAI-compatible adapter + transforms
â”œâ”€â”€ openai/
â”‚   â”œâ”€â”€ text-generate/
â”‚   â”œâ”€â”€ image-generate/
â”‚   â””â”€â”€ audio-generate/
â”œâ”€â”€ google/
â”‚   â”œâ”€â”€ text-generate/
â”‚   â””â”€â”€ video-generate/
â”œâ”€â”€ google-ai-studio/
â”‚   â””â”€â”€ text-generate/
â”œâ”€â”€ anthropic/
â”‚   â””â”€â”€ text-generate/
â”œâ”€â”€ fal/
â”‚   â”œâ”€â”€ image-generate/
â”‚   â”œâ”€â”€ video-generate/
â”‚   â””â”€â”€ audio-generate/
â””â”€â”€ ... (one folder per provider)
```

---

## ğŸ”§ Configuration

### Environment Variables

#### Text Generation

```bash
# Core Providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=...
GOOGLE_AI_STUDIO_API_KEY=...

# New High-Performance Providers
GROQ_API_KEY=gsk_...
CEREBRAS_API_KEY=csk_...
TOGETHER_API_KEY=...
FIREWORKS_API_KEY=...

# Chinese Providers
DEEPSEEK_API_KEY=sk-...
XIAOMI_MIMO_API_KEY=...
MINIMAX_API_KEY=...
QWEN_API_KEY=...
MOONSHOT_AI_API_KEY=...

# International Providers
XAI_API_KEY=xai-...
MISTRAL_API_KEY=...
COHERE_API_KEY=...
PERPLEXITY_API_KEY=pplx-...
LIQUID_AI_API_KEY=...

# Infrastructure Providers
DEEPINFRA_API_KEY=...
NOVITA_API_KEY=...
BASETEN_API_KEY=...
```

#### Multi-Modal Providers

```bash
# OpenAI (Image, Audio)
OPENAI_API_KEY=sk-...

# Google (Video, Text+Image via Nano Banana)
GOOGLE_API_KEY=...
GOOGLE_AI_STUDIO_API_KEY=...

# Fal.ai (Image, Video, Audio)
FAL_API_KEY=...
```

---

## ğŸ“š Documentation Sources

All implementations verified against latest 2026 provider documentation:

### Core Providers
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Anthropic Messages API](https://docs.anthropic.com/en/docs/build-with-claude)
- [Google Gemini API](https://ai.google.dev/gemini-api/docs)

### High-Performance Providers
- [Groq OpenAI Compatibility](https://console.groq.com/docs/openai)
- [Together AI OpenAI Compatibility](https://docs.together.ai/docs/openai-api-compatibility)
- [Fireworks AI OpenAI Compatibility](https://docs.fireworks.ai/tools-sdks/openai-compatibility)
- [Cerebras Inference Docs](https://inference-docs.cerebras.ai/introduction)

### Chinese Providers
- [DeepSeek Thinking Mode](https://api-docs.deepseek.com/guides/thinking_mode)
- [Xiaomi MiMo GitHub](https://github.com/XiaomiMiMo/MiMo-V2-Flash)
- [MiniMax Tool Use & Thinking](https://platform.minimax.io/docs/guides/text-m2-function-call)
- [Z.AI Thinking Mode](https://docs.z.ai/guides/capabilities/thinking-mode)

### Multi-Modal
- [Fal.ai Documentation](https://docs.fal.ai/)
- [Fal.ai Video Models](https://fal.ai/video)

---

## âœ… Testing Checklist

### Text Generation
- [x] OpenAI GPT-4o with reasoning
- [x] Anthropic Claude 3.5 Sonnet with extended thinking
- [x] Google Gemini 2.0 Flash Thinking
- [x] Groq Llama models (ultra-fast)
- [x] DeepSeek V3.2 with thinking mode
- [x] Xiaomi MiMo with enable_thinking
- [x] MiniMax M2.1 with reasoning_content

### Image Generation
- [ ] Fal FLUX/schnell
- [ ] Fal SDXL
- [ ] Fal Stable Diffusion V3

### Video Generation
- [ ] Fal Veo 3 text-to-video
- [ ] Fal Hunyuan Video
- [ ] Fal Veo 2 image-to-video

### Audio Generation
- [ ] Fal TTS
- [ ] Fal Music Generation

---

## ğŸš€ Next Steps

1. **Add More Image Providers**: Stability AI, Midjourney API (when available)
2. **Add More Video Providers**: Runway ML, Pika Labs
3. **Add More Audio Providers**: ElevenLabs, OpenAI TTS, Suno
4. **Implement Pricing**: Add cost tracking for all new providers
5. **Add Provider Health Checks**: Monitor availability and latency
6. **Create Integration Tests**: End-to-end tests for each capability

---

## ğŸ“ Notes

`â˜… Insight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`
**Multi-Modal Architecture**: The executor system now supports four modalities (text, image, video, audio) through a unified capability-based resolution system. Each capability has its own IR format and executor registry, allowing providers to specialize in their strengths while maintaining a consistent API surface for clients.

**Provider Flexibility**: The OpenAI-compatible executor pattern allows rapid onboarding of new text providers (14 added in this update) with minimal code - just a simple wrapper pointing to the shared executor logic. Only providers with special quirks (like Xiaomi's `chat_template_kwargs` or DeepSeek's `reasoning_content`) need custom handling.

**Fal Integration**: Fal.ai provides access to 600+ models across three modalities through a single unified API, making it an ideal foundation for image/video/audio generation capabilities. The executor design allows easy expansion to other providers (Runway, Stability, etc.) following the same pattern.
`â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`

