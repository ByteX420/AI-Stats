/**
 * Benchmark identifier.
 */
export type BenchmarkId =
  | "ace-bench"
  | "ai2-sciarena"
  | "ai2d"
  | "aidanbench"
  | "aider-polyglot"
  | "aime-2024"
  | "aime-2025"
  | "amc"
  | "arc-agi-1"
  | "arc-agi-2"
  | "arena-hard"
  | "autologi"
  | "balrog-ai"
  | "bfcl-overall-fc-v4"
  | "bigcodebench"
  | "browsecomp"
  | "browsecomp-long-context-128k"
  | "browsecomp-long-context-256k"
  | "c-eval"
  | "chartqa"
  | "charxiv-reasoning"
  | "cnmo-2024"
  | "codeforces"
  | "collie"
  | "confabulations"
  | "creative-story-writing"
  | "csimpleqa"
  | "docvqa"
  | "dubesor-llm"
  | "elimination-game"
  | "eqbench"
  | "erqa"
  | "evalplus"
  | "facts"
  | "facts-benchmark-suite"
  | "factscore-halluciation-rate"
  | "fiction-live-bench"
  | "frontier-math"
  | "galileo-agent"
  | "global-pica"
  | "gpqa"
  | "gpqa-diamond"
  | "graphwalks-bfs-lt-128k"
  | "graphwalks-parents-lt-128k"
  | "gsm8k"
  | "healthbench"
  | "healthbench-concensus"
  | "healthbench-hard"
  | "hmmt-2025"
  | "humaneval"
  | "humanitys-last-exam"
  | "if-bench"
  | "if-eval"
  | "imoanswerbench"
  | "iq-bench"
  | "lisanbench"
  | "livebench"
  | "livecodebench"
  | "livecodebench-coding"
  | "livecodebench-pro"
  | "livecodebench-v5"
  | "livecodebench-v6"
  | "lmarena-text"
  | "lmarena-webdev"
  | "longcodebench-1m"
  | "longfact-concepts-hallucination-rate"
  | "longfact-objects-hallucination-rate"
  | "math"
  | "math-500"
  | "matharena"
  | "matharena-apex"
  | "mathvista"
  | "mc-bench"
  | "metr"
  | "misguided-attention"
  | "mle-bench"
  | "mm-mt-bench"
  | "mmlu"
  | "mmlu-multilingual"
  | "mmlu-pro"
  | "mmlu-redux"
  | "mmlu-redux-2.0"
  | "mmmlu"
  | "mmmu"
  | "mmmu-pro"
  | "multi-challenge"
  | "multiPL-E"
  | "nyt-connections"
  | "ocrbench-v2"
  | "ojbench"
  | "omnidocbench-1.5"
  | "openai-mrcr-2-needle-128k"
  | "openai-mrcr-2-needle-256k"
  | "openai-mrcr-8-needle-128k"
  | "openai-mrcr-8-needle-1m"
  | "os-world"
  | "paperbench"
  | "phybench"
  | "polymath-en"
  | "qvhighlights"
  | "realkie"
  | "scale-mcp-atlas"
  | "screenspot"
  | "screenspot-pro"
  | "seal-multichallenege"
  | "simplebench"
  | "simpleqa"
  | "smolagents-llm"
  | "snake-bench"
  | "solo-bench"
  | "supergpqa"
  | "swe-bench"
  | "swe-bench-live"
  | "swe-bench-multilingual"
  | "swe-bench-pro"
  | "swe-lancer"
  | "symflower-coding"
  | "tau-2-airline"
  | "tau-2-bench"
  | "tau-2-retail"
  | "tau-2-telecom"
  | "tau-bench"
  | "tau-bench-airline"
  | "tau-bench-retail"
  | "terminal-bench"
  | "terminal-bench-2.0"
  | "thematic-generalisation"
  | "triviaqa"
  | "usamo-2025"
  | "vending-bench-2"
  | "video-mmmu"
  | "videomme"
  | "weirdml"
  | "wildbench"
  | "xlang-agent"
  | "zebralogic";
